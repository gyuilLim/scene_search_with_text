{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYuXOUcJ_DpZ"
      },
      "source": [
        "# Setting Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_liJoqsh_EbY",
        "outputId": "edd6e053-b204-43e0-e11e-f1fca9557904"
      },
      "outputs": [],
      "source": [
        "!pip install pytube\n",
        "!pip install peakutils\n",
        "!pip install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install salesforce-lavis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGrDtrP4_EE9"
      },
      "source": [
        "# Input Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMU4i3IRwkSd"
      },
      "outputs": [],
      "source": [
        "youtube_url = 'https://www.youtube.com/watch?v=82C19hXaloc'\n",
        "text = 'A man standing while holding a book'\n",
        "model_name = 'clip'\n",
        "kfe = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLP5mWSq6zua"
      },
      "source": [
        "# Video Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q-nolx_Vzwgt",
        "outputId": "9e98a3fa-8e43-472e-97b1-f95293b83eef"
      },
      "outputs": [],
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "streams = YouTube(youtube_url).streams.filter(adaptive=True, subtype=\"mp4\", resolution=\"360p\", only_video=True)\n",
        "\n",
        "streams[0].download(filename=\"video.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7A-rPvOwkSe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import re\n",
        "\n",
        "def extract_video_id(youtube_link):\n",
        "    index = youtube_link.find('?')\n",
        "\n",
        "    if index != -1 :\n",
        "        youtube_link = youtube_link[:index]\n",
        "\n",
        "    return youtube_link\n",
        "\n",
        "def video_load_from_url(url) :\n",
        "    url = extract_video_id(url)\n",
        "\n",
        "    cap = cv2.VideoCapture('./video.mp4')\n",
        "\n",
        "    return cap, url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj9dFx8LwkSf",
        "outputId": "b964cd4f-d02e-4e0a-9def-16fbe12e945f"
      },
      "outputs": [],
      "source": [
        "# video load\n",
        "print('video load ... ')\n",
        "cap, url = video_load_from_url(youtube_url)\n",
        "print('video load complete ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XK2WDqdwkSf"
      },
      "source": [
        "# Key Frame Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6t-7L9mwkSf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "import numpy as np\n",
        "import time\n",
        "import peakutils\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def scale(img, xScale, yScale):\n",
        "    res = cv2.resize(img, None, fx=xScale, fy=yScale, interpolation=cv2.INTER_AREA)\n",
        "    return res\n",
        "\n",
        "\n",
        "def convert_frame_to_grayscale(frame):\n",
        "    grayframe = None\n",
        "    gray = None\n",
        "    if frame is not None:\n",
        "        cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        gray = scale(gray, 1, 1)\n",
        "        grayframe = scale(gray, 1, 1)\n",
        "        gray = cv2.GaussianBlur(gray, (9, 9), 0.0)\n",
        "    return grayframe, gray\n",
        "\n",
        "\n",
        "def key_frame_detection(cap, kfe=False, Thres=0.3, plotMetrics=False, verbose=False):\n",
        "\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    spf = 1 / fps\n",
        "\n",
        "    if kfe == False :\n",
        "        frame_list = []\n",
        "        time_spans = []\n",
        "        for i in tqdm(range(0, length)) :\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if i % 12 == 0 :\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame_list.append(frame)\n",
        "                time_spans.append(spf * i)\n",
        "\n",
        "        return frame_list, time_spans\n",
        "\n",
        "    if (cap.isOpened()== False):\n",
        "        print(\"Error opening video file\")\n",
        "\n",
        "    lstfrm = []\n",
        "    lstdiffMag = []\n",
        "    timeSpans = []\n",
        "    images = []\n",
        "    full_color = []\n",
        "    lastFrame = None\n",
        "\n",
        "    # Read until video is completed\n",
        "    for i in tqdm(range(length)):\n",
        "        ret, frame = cap.read()\n",
        "        grayframe, blur_gray = convert_frame_to_grayscale(frame)\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        frame_number = cap.get(cv2.CAP_PROP_POS_FRAMES) - 1\n",
        "        lstfrm.append(frame_number)\n",
        "        images.append(grayframe)\n",
        "        full_color.append(frame)\n",
        "        if frame_number == 0:\n",
        "            lastFrame = blur_gray\n",
        "\n",
        "        diff = cv2.subtract(blur_gray, lastFrame)\n",
        "        diffMag = cv2.countNonZero(diff)\n",
        "        lstdiffMag.append(diffMag)\n",
        "        time_Span = i * spf\n",
        "        timeSpans.append(time_Span)\n",
        "        lastFrame = blur_gray\n",
        "\n",
        "    cap.release()\n",
        "    y = np.array(lstdiffMag)\n",
        "    base = peakutils.baseline(y, 2)\n",
        "    indices = peakutils.indexes(y-base, Thres, min_dist=1)\n",
        "\n",
        "    return_keyframe = []\n",
        "    return_frame_times = []\n",
        "\n",
        "    for x in indices:\n",
        "        return_keyframe.append(full_color[x])\n",
        "        return_frame_times.append(timeSpans[x])  # 프레임 시간 추가\n",
        "\n",
        "    return return_keyframe, return_frame_times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaiJELjgwkSg",
        "outputId": "9133f2d7-1249-45e9-fb4e-58375197d1db"
      },
      "outputs": [],
      "source": [
        "# KFE\n",
        "print('key frame extraction ... ')\n",
        "frame_list, time_line = key_frame_detection(cap, kfe)\n",
        "print('key frame extraction complete ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0w-tQ1DwkSg"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiF5pkbhwkSg"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class custom_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, frame_list, model):\n",
        "        self.frame_list = frame_list\n",
        "        self.model = model\n",
        "        self.pil_image_list = [self.frame_to_PIL(frame) for frame in frame_list]\n",
        "\n",
        "        self.image_size = 224\n",
        "\n",
        "        self.mean = (0.48145466, 0.4578275, 0.40821073)\n",
        "        self.std = (0.26862954, 0.26130258, 0.27577711)\n",
        "\n",
        "        self.transform = transforms.Compose(\n",
        "                [\n",
        "                    transforms.Resize(\n",
        "                        (self.image_size, self.image_size), interpolation=InterpolationMode.BICUBIC\n",
        "                    ),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(self.mean, self.std),\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pil_image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.transform(self.pil_image_list[idx])\n",
        "\n",
        "    def frame_to_PIL(self, frame):\n",
        "        pil_image = Image.fromarray(frame)\n",
        "        return pil_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH-JcKIYwkSh"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# dataloader\n",
        "dataset = custom_dataset(frame_list, model_name)\n",
        "dataloader = DataLoader(dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FO5RIVHwkSh"
      },
      "source": [
        "# Model Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLJVrripwkSh"
      },
      "outputs": [],
      "source": [
        "from lavis.models import load_model_and_preprocess\n",
        "import clip\n",
        "\n",
        "def load_blip(device) :\n",
        "    model, _, _ = load_model_and_preprocess(\n",
        "        name=\"blip2\", model_type=\"pretrain\", is_eval=True, device=device\n",
        "    )\n",
        "\n",
        "    return model.float()\n",
        "\n",
        "def load_clip(device) :\n",
        "    model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_model(name, device) :\n",
        "    if name == \"clip\" :\n",
        "        return load_clip(device)\n",
        "\n",
        "    elif name == \"blip\" :\n",
        "        return load_blip(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVuZC0_XwkSh",
        "outputId": "91495cf8-182c-4e95-9a5d-76afb72a7398"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "print('model load ...')\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = load_model(model_name, device)\n",
        "print('model load complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U31d4V2E_rZ9"
      },
      "source": [
        "# Text Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4glGL9S_rfF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2\"\n",
        "api_token = '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'\n",
        "headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
        "\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "def text_similarity(flat_msg_list, pormpt) :\n",
        "    data = query(\n",
        "        {\n",
        "            \"inputs\": {\n",
        "                \"source_sentence\": pormpt,\n",
        "                \"sentences\": flat_msg_list\n",
        "            }\n",
        "        })\n",
        "\n",
        "    max_index = np.argmax(data)\n",
        "\n",
        "    return max_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC55D_qZwkSh"
      },
      "source": [
        "# Infrence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg5QI4slwkSh"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "from tqdm import tqdm\n",
        "\n",
        "def inference(model_name, text, model, dataloader, device) :\n",
        "    if model_name == \"clip\" :\n",
        "        logits = []\n",
        "        text = clip.tokenize([text]).to(device)\n",
        "        for data in tqdm(dataloader) :\n",
        "            data = data.to(device)\n",
        "            image_features = model.encode_image(data)\n",
        "            text_features = model.encode_text(text)\n",
        "\n",
        "            logits_per_image, logits_per_text = model(data, text)\n",
        "            logits.append(logits_per_image)\n",
        "\n",
        "            flat = [item.item() for logit in logits for item in logit]\n",
        "            max_index = flat.index(max(flat))\n",
        "\n",
        "\n",
        "    elif model_name == \"blip\" :\n",
        "        msg_list = []\n",
        "        for data in tqdm(dataloader) :\n",
        "            data = data.to(device)\n",
        "            msg = model.generate({\"image\": data})\n",
        "            msg_list.append(msg)\n",
        "\n",
        "        flat_msg_list = [msg for sublist in msg_list for msg in sublist]\n",
        "        max_index = text_similarity(flat_msg_list, text)\n",
        "\n",
        "\n",
        "    return max_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyQa4rjHwkSi",
        "outputId": "1a20fad6-574a-440e-9944-d157a7dc36f0"
      },
      "outputs": [],
      "source": [
        "# inference\n",
        "print('inference loading ... ')\n",
        "model.eval()\n",
        "max_index = inference(model_name, text, model, dataloader, device)\n",
        "print('infrence complete ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOf-kIyRwkSi"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "HCAp1C3IwkSi",
        "outputId": "b1c1eade-c757-4cd6-d83d-cc42a7915aa1"
      },
      "outputs": [],
      "source": [
        "print(youtube_url + '&t=' + str(int(time_line[max_index])))\n",
        "plt.imshow(frame_list[max_index])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
